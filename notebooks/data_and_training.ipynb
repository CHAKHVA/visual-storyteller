{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning: Data Exploration and Training\n\nThis notebook explores the Flickr8k dataset and trains the image captioning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\nfrom pathlib import Path\n\n# Add project root to path\nproject_root = Path().resolve().parent\nsys.path.insert(0, str(project_root))\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom PIL import Image\nfrom collections import Counter\n\nfrom src.utils import load_config, get_device, parse_captions_file, set_seed\nfrom src.vocab import Vocabulary\nfrom src.dataset import build_vocab_from_dataloader, get_dataloaders, get_transforms\nfrom src.model import ImageCaptioningModel\nfrom src.train import train\n\n%matplotlib inline\nplt.style.use('seaborn-v0_8-darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\nconfig = load_config(\"../configs/config.yaml\")\nprint(\"Configuration loaded successfully!\")\nprint(f\"Batch size: {config['data']['batch_size']}\")\nprint(f\"Vocab frequency threshold: {config['data']['freq_threshold']}\")\nprint(f\"Max caption length: {config['data']['max_caption_length']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device and seed\ndevice = get_device()\nset_seed(42)\nprint(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load captions\ncaptions_df = parse_captions_file(config[\"data\"][\"captions_file\"])\nprint(f\"Loaded {len(captions_df)} captions\")\ncaptions_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset statistics\nnum_unique_images = captions_df['image'].nunique()\ntotal_captions = len(captions_df)\ncaptions_per_image = captions_df.groupby('image').size()\n\nprint(\"=\" * 50)\nprint(\"Dataset Statistics\")\nprint(\"=\" * 50)\nprint(f\"Number of unique images: {num_unique_images}\")\nprint(f\"Total number of captions: {total_captions}\")\nprint(f\"\\nCaptions per image distribution:\")\nprint(captions_per_image.value_counts().sort_index())\nprint(f\"\\nMean captions per image: {captions_per_image.mean():.2f}\")\nprint(f\"Median captions per image: {captions_per_image.median():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caption length distribution\ncaptions_df['caption_length'] = captions_df['caption'].apply(lambda x: len(x.split()))\n\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.hist(captions_df['caption_length'], bins=30, edgecolor='black', alpha=0.7)\nplt.xlabel('Caption Length (words)')\nplt.ylabel('Frequency')\nplt.title('Distribution of Caption Lengths')\nplt.axvline(captions_df['caption_length'].mean(), color='red', linestyle='--', \n            label=f\"Mean: {captions_df['caption_length'].mean():.1f}\")\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.boxplot(captions_df['caption_length'])\nplt.ylabel('Caption Length (words)')\nplt.title('Caption Length Box Plot')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nCaption length statistics:\")\nprint(f\"  Mean: {captions_df['caption_length'].mean():.2f}\")\nprint(f\"  Median: {captions_df['caption_length'].median():.0f}\")\nprint(f\"  Min: {captions_df['caption_length'].min()}\")\nprint(f\"  Max: {captions_df['caption_length'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word frequency distribution\nall_words = []\nfor caption in captions_df['caption']:\n    words = caption.lower().split()\n    all_words.extend(words)\n\nword_counts = Counter(all_words)\nmost_common_words = word_counts.most_common(50)\n\nwords, counts = zip(*most_common_words)\n\nplt.figure(figsize=(15, 6))\nplt.bar(range(len(words)), counts, alpha=0.7)\nplt.xticks(range(len(words)), words, rotation=45, ha='right')\nplt.xlabel('Words')\nplt.ylabel('Frequency')\nplt.title('Top 50 Most Frequent Words')\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nTotal unique words: {len(word_counts)}\")\nprint(f\"Total word occurrences: {len(all_words)}\")\nprint(f\"\\nTop 10 words:\")\nfor word, count in most_common_words[:10]:\n    print(f\"  {word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display random images with their captions\nimage_dir = Path(config[\"data\"][\"image_dir\"])\n\n# Sample 5 random images\nrandom_images = captions_df['image'].unique()[:5]\n\nfig, axes = plt.subplots(1, 5, figsize=(20, 5))\n\nfor idx, img_name in enumerate(random_images):\n    # Load image\n    img_path = image_dir / img_name\n    if img_path.exists():\n        img = Image.open(img_path).convert('RGB')\n        axes[idx].imshow(img)\n        axes[idx].axis('off')\n        \n        # Get captions for this image\n        img_captions = captions_df[captions_df['image'] == img_name]['caption'].tolist()\n        \n        # Create title with captions\n        title = f\"{img_name}\\n\"\n        for i, cap in enumerate(img_captions[:3], 1):  # Show first 3 captions\n            title += f\"{i}. {cap[:30]}...\\n\"\n        axes[idx].set_title(title, fontsize=8)\n\nplt.tight_layout()\nplt.show()\n\n# Print all captions for one image\nprint(f\"\\nAll captions for {random_images[0]}:\")\nfor i, cap in enumerate(captions_df[captions_df['image'] == random_images[0]]['caption'], 1):\n    print(f\"  {i}. {cap}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vocabulary Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary\nprint(\"Building vocabulary...\")\nvocab = build_vocab_from_dataloader(captions_df, config[\"data\"][\"freq_threshold\"])\n\nprint(f\"\\nVocabulary size: {len(vocab)}\")\nprint(f\"Frequency threshold: {vocab.freq_threshold}\")\nprint(f\"\\nSpecial tokens:\")\nprint(f\"  <PAD>: {vocab.stoi['<PAD>']}\")\nprint(f\"  <SOS>: {vocab.stoi['<SOS>']}\")\nprint(f\"  <EOS>: {vocab.stoi['<EOS>']}\")\nprint(f\"  <UNK>: {vocab.stoi['<UNK>']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coverage statistics\n# Count how many words are mapped to <UNK>\nunk_count = 0\ntotal_tokens = 0\n\nfor caption in captions_df['caption']:\n    tokens = caption.lower().split()\n    total_tokens += len(tokens)\n    for token in tokens:\n        if token not in vocab.stoi:\n            unk_count += 1\n\ncoverage = (1 - unk_count / total_tokens) * 100\n\nprint(f\"Vocabulary Coverage:\")\nprint(f\"  Total tokens: {total_tokens}\")\nprint(f\"  UNK tokens: {unk_count}\")\nprint(f\"  Coverage: {coverage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example numericalize/denumericalize\ntest_captions = [\n    \"a dog is running in the park\",\n    \"two people walking on the beach\",\n    \"a cat sitting on a couch\",\n]\n\nprint(\"Numericalize/Denumericalize Examples:\")\nprint(\"=\" * 80)\n\nfor caption in test_captions:\n    # Numericalize\n    indices = vocab.numericalize(caption)\n    \n    # Denumericalize\n    reconstructed = vocab.denumericalize(indices)\n    \n    print(f\"\\nOriginal:      {caption}\")\n    print(f\"Indices:       {indices}\")\n    print(f\"Reconstructed: {reconstructed}\")\n    print(f\"Match: {caption == reconstructed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\nprint(\"Creating dataloaders...\")\ntrain_loader, val_loader, test_loader = get_dataloaders(config, vocab)\n\nprint(f\"\\nDataLoader Statistics:\")\nprint(f\"  Train batches: {len(train_loader)}\")\nprint(f\"  Val batches: {len(val_loader)}\")\nprint(f\"  Test batches: {len(test_loader)}\")\nprint(f\"  Batch size: {config['data']['batch_size']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one batch\ntrain_iter = iter(train_loader)\nimages, captions, image_names = next(train_iter)\n\nprint(f\"Batch shapes:\")\nprint(f\"  Images: {images.shape}  # (batch, channels, height, width)\")\nprint(f\"  Captions: {captions.shape}  # (seq_len, batch) - time-first format\")\nprint(f\"  Image names: {len(image_names)}\")\n\nprint(f\"\\nImage statistics:\")\nprint(f\"  Mean: {images.mean():.3f}\")\nprint(f\"  Std: {images.std():.3f}\")\nprint(f\"  Min: {images.min():.3f}\")\nprint(f\"  Max: {images.max():.3f}\")\nprint(f\"  (Images are normalized with ImageNet stats)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize batch\n# Denormalize images for visualization\nmean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\nstd = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n\ndef denormalize(img_tensor):\n    \"\"\"Denormalize image tensor for visualization.\"\"\"\n    img = img_tensor * std + mean\n    img = torch.clamp(img, 0, 1)\n    return img\n\n# Display 4 images with captions\nfig, axes = plt.subplots(2, 2, figsize=(12, 12))\naxes = axes.ravel()\n\nfor idx in range(4):\n    # Denormalize and convert to numpy\n    img = denormalize(images[idx]).permute(1, 2, 0).numpy()\n    \n    # Get caption (transpose to batch-first)\n    caption_indices = captions[:, idx].tolist()\n    caption_text = vocab.denumericalize(caption_indices)\n    \n    # Display\n    axes[idx].imshow(img)\n    axes[idx].axis('off')\n    axes[idx].set_title(f\"{image_names[idx]}\\n{caption_text}\", fontsize=10)\n\nplt.tight_layout()\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\nprint(\"Creating model...\")\nmodel = ImageCaptioningModel.create_from_config(config, vocab_size=len(vocab))\nmodel = model.to(device)\n\nprint(\"\\nModel created successfully!\")\nprint(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture summary\nprint(\"=\" * 80)\nprint(\"Model Architecture\")\nprint(\"=\" * 80)\n\nprint(\"\\nEncoder (CNN):\")\nprint(f\"  Backbone: {config['model']['encoder_backbone']}\")\nprint(f\"  Output: (batch, 49, {config['model']['embed_size']})\")\nprint(f\"  Features from: 7x7 spatial grid\")\n\nprint(\"\\nDecoder (Transformer):\")\nprint(f\"  Embed size: {config['model']['embed_size']}\")\nprint(f\"  Num heads: {config['model']['num_heads']}\")\nprint(f\"  Num layers: {config['model']['num_layers']}\")\nprint(f\"  Dropout: {config['model']['dropout']}\")\nprint(f\"  Vocab size: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter counts\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nfrozen_params = total_params - trainable_params\n\nencoder_params = sum(p.numel() for p in model.encoder.parameters())\ndecoder_params = sum(p.numel() for p in model.decoder.parameters())\n\nprint(\"=\" * 80)\nprint(\"Parameter Counts\")\nprint(\"=\" * 80)\n\nprint(f\"\\nTotal parameters: {total_params:,}\")\nprint(f\"  Trainable: {trainable_params:,} ({trainable_params/total_params*100:.1f}%)\")\nprint(f\"  Frozen: {frozen_params:,} ({frozen_params/total_params*100:.1f}%)\")\n\nprint(f\"\\nBy component:\")\nprint(f\"  Encoder: {encoder_params:,}\")\nprint(f\"  Decoder: {decoder_params:,}\")\n\nprint(f\"\\nNote: Encoder is initially frozen and will be unfrozen at epoch {config['training']['unfreeze_encoder_epoch']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training\n\n**Note:** Training takes significant time. You can reduce `epochs` in config for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n# This will take a while depending on your hardware\nprint(\"Starting training...\")\nprint(\"This will save checkpoints to: checkpoints/best_model.pt\")\nprint()\n\nbest_model_path = train(\"../configs/config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Loss curves are typically logged during training\n# If you want to plot them, you can modify the train function to return loss history\n# or use tensorboard/wandb for tracking\n\nprint(\"Training completed!\")\nprint(f\"Best model saved to: {best_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check saved artifacts\ncheckpoint_dir = Path(\"../checkpoints\")\n\nprint(\"=\" * 80)\nprint(\"Saved Artifacts\")\nprint(\"=\" * 80)\n\n# Check for saved files\nbest_model = checkpoint_dir / \"best_model.pt\"\nvocab_file = checkpoint_dir / \"vocab.pkl\"\n\nif best_model.exists():\n    size_mb = best_model.stat().st_size / (1024 * 1024)\n    print(f\"\u2713 Best model: {best_model}\")\n    print(f\"  Size: {size_mb:.2f} MB\")\nelse:\n    print(f\"\u2717 Best model not found: {best_model}\")\n\nif vocab_file.exists():\n    size_kb = vocab_file.stat().st_size / 1024\n    print(f\"\\n\u2713 Vocabulary: {vocab_file}\")\n    print(f\"  Size: {size_kb:.2f} KB\")\nelse:\n    print(f\"\\n\u2717 Vocabulary not found: {vocab_file}\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Next steps:\")\nprint(\"  1. Run inference.ipynb to test the model\")\nprint(\"  2. Run: python -m src.evaluate to get BLEU scores\")\nprint(\"  3. Visualize results in the inference notebook\")\nprint(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
